{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all files\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numba import jit\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Set constants for function uses\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "DIR_GLOVE = 'glove/glove.840B.300d/'\n",
    "DIR_DATA = 'data/'\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "TEST_SPLIT = 0.1\n",
    "VALIDATION_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply regex to clean the string\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "# Uses glove vectorizer for making word embeddings\n",
    "def gloveVec(filename):\n",
    "    embeddings = {}\n",
    "    f = open(os.path.join(filename), encoding='utf-8')\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = coefs\n",
    "        except ValueError:\n",
    "            i += 1\n",
    "    f.close()\n",
    "    return embeddings\n",
    "\n",
    "# Data preprocessing, add labels from dataset\n",
    "def loadData(filename):\n",
    "    df = pd.read_csv(DIR_DATA + filename)\n",
    "    selected = ['label', 'text']\n",
    "    non_selected = list(set(df.columns) - set(selected))\n",
    "    df = df.drop(non_selected, axis=1)\n",
    "    df = df.dropna(axis=0, how='any', subset=selected)\n",
    "    labels = sorted(list(set(df[selected[0]].tolist())))\n",
    "    dict.fromkeys(set(df[selected[0]].tolist()))\n",
    "    label_dict = {}\n",
    "    for i in range(len(labels)):\n",
    "        label_dict[labels[i]] = i\n",
    "\n",
    "    x_train = df[selected[1]].apply(lambda x: clean_str(x)).tolist()\n",
    "    y_train = df[selected[0]].apply(lambda y: label_dict[y]).tolist()\n",
    "    y_train = to_categorical(np.asarray(y_train))\n",
    "    return x_train,y_train,labels\n",
    "\n",
    "# Tokenizes sentences from the dataset using keras tokenizer\n",
    "def createVocabAndData(sentences):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    vocab = tokenizer.word_index\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    return vocab,data,tokenizer\n",
    "\n",
    "#Ensures that all sentences have the same length\n",
    "def oneSentence(sentence, tokenizer):\n",
    "    sequences = tokenizer.texts_to_sequences(sentence)\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    return sequences,data\n",
    "\n",
    "# Uses to create vocabulary or Embedding matrix for  dataset\n",
    "def createEmbeddingMatrix(word_index,embeddings_index):\n",
    "    nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "    embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i > MAX_NB_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def lstmModel(embedding_matrix,epoch,no_of_emotions):\n",
    "    model = Sequential()\n",
    "    n, embedding_dims = embedding_matrix.shape\n",
    "\n",
    "    model.add(Embedding(n, embedding_dims, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "    model.add(LSTM(128, dropout=0.6, recurrent_dropout=0.6))\n",
    "    model.add(Dense(no_of_emotions))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_split=VALIDATION_SPLIT, epochs=epoch, batch_size=128)\n",
    "    model.save_weights('text_lstm_weights.h5')\n",
    "    model.save('lstm_tweets_model.h5')\n",
    "    scores= model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data created\n",
      "Train Test split\n"
     ]
    }
   ],
   "source": [
    "sentences, labels, label_dict = loadData('tweet_emotions.csv')\n",
    "f1 = open('embedding_matrix.pkl','rb')\n",
    "data,labels,embedding_mat = pickle.load(f1)\n",
    "# embeddings = gloveVec('glove/glove.840B.300d.txt')\n",
    "vocab, data, tokenizer = createVocabAndData(sentences)\n",
    "# embedding_mat = createEmbeddingMatrix(vocab,embeddings)\n",
    "no_of_emotions = len(labels[0])\n",
    "\n",
    "\n",
    "\n",
    "# pickle.dump([data, labels, embedding_mat], open('embedding_matrix.pkl', 'wb'))\n",
    "print (\"Data created\")\n",
    "\n",
    "print(\"Train Test split\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=TEST_SPLIT, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_json = tokenizer.to_json()\n",
    "with open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.18732999,  0.40595001, -0.51174003, ...,  0.16495   ,\n",
       "         0.18757001,  0.53873998],\n",
       "       [ 0.31924   ,  0.06316   , -0.27858001, ...,  0.082745  ,\n",
       "         0.097801  ,  0.25044999],\n",
       "       ...,\n",
       "       [ 0.81659001, -0.9479    , -0.28915   , ..., -0.25492001,\n",
       "        -0.46274   ,  0.28736001],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.39831001, -0.1101    , -0.55002999, ...,  0.32887   ,\n",
       "         0.25922   ,  0.54447001]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1,\n",
       " 'to': 2,\n",
       " 'the': 3,\n",
       " 'a': 4,\n",
       " 'my': 5,\n",
       " 'you': 6,\n",
       " 'it': 7,\n",
       " 'and': 8,\n",
       " 'is': 9,\n",
       " 'in': 10,\n",
       " 'for': 11,\n",
       " \"'s\": 12,\n",
       " 'of': 13,\n",
       " \"n't\": 14,\n",
       " 'that': 15,\n",
       " 'me': 16,\n",
       " 'on': 17,\n",
       " 'have': 18,\n",
       " 'so': 19,\n",
       " 'but': 20,\n",
       " 'just': 21,\n",
       " \"i'm\": 22,\n",
       " 'day': 23,\n",
       " 'with': 24,\n",
       " 'do': 25,\n",
       " 'be': 26,\n",
       " 'was': 27,\n",
       " 'at': 28,\n",
       " 'not': 29,\n",
       " 'good': 30,\n",
       " 'all': 31,\n",
       " 'this': 32,\n",
       " 'now': 33,\n",
       " 'out': 34,\n",
       " 'up': 35,\n",
       " 'get': 36,\n",
       " 'are': 37,\n",
       " 'like': 38,\n",
       " 'no': 39,\n",
       " 'quot': 40,\n",
       " 'http': 41,\n",
       " 'go': 42,\n",
       " 'your': 43,\n",
       " 'today': 44,\n",
       " 'love': 45,\n",
       " 'work': 46,\n",
       " 'too': 47,\n",
       " 'got': 48,\n",
       " 'going': 49,\n",
       " 'we': 50,\n",
       " 'happy': 51,\n",
       " 'what': 52,\n",
       " 'lol': 53,\n",
       " 'from': 54,\n",
       " 'one': 55,\n",
       " 'time': 56,\n",
       " 'u': 57,\n",
       " 'know': 58,\n",
       " 'back': 59,\n",
       " 'com': 60,\n",
       " 'im': 61,\n",
       " 'will': 62,\n",
       " 'there': 63,\n",
       " 'really': 64,\n",
       " 'am': 65,\n",
       " 'about': 66,\n",
       " 'its': 67,\n",
       " 'see': 68,\n",
       " 'had': 69,\n",
       " 'amp': 70,\n",
       " 'did': 71,\n",
       " 'can': 72,\n",
       " 'ca': 73,\n",
       " 'they': 74,\n",
       " 'some': 75,\n",
       " 'night': 76,\n",
       " 'if': 77,\n",
       " 'new': 78,\n",
       " 'home': 79,\n",
       " 'how': 80,\n",
       " 'think': 81,\n",
       " 'well': 82,\n",
       " '2': 83,\n",
       " 'thanks': 84,\n",
       " 'when': 85,\n",
       " 'want': 86,\n",
       " 'as': 87,\n",
       " 'oh': 88,\n",
       " \"'ll\": 89,\n",
       " 'still': 90,\n",
       " 'off': 91,\n",
       " 'he': 92,\n",
       " 'much': 93,\n",
       " 'here': 94,\n",
       " 'an': 95,\n",
       " 'more': 96,\n",
       " 'miss': 97,\n",
       " 'great': 98,\n",
       " 'last': 99,\n",
       " 'need': 100,\n",
       " 'morning': 101,\n",
       " 'hope': 102,\n",
       " 'has': 103,\n",
       " 'her': 104,\n",
       " 'twitter': 105,\n",
       " 'haha': 106,\n",
       " 'been': 107,\n",
       " 'would': 108,\n",
       " \"'re\": 109,\n",
       " 'she': 110,\n",
       " 'then': 111,\n",
       " 'feel': 112,\n",
       " 'fun': 113,\n",
       " 'or': 114,\n",
       " 'again': 115,\n",
       " '3': 116,\n",
       " 'only': 117,\n",
       " 'sad': 118,\n",
       " 'why': 119,\n",
       " 'tomorrow': 120,\n",
       " 'wish': 121,\n",
       " 'could': 122,\n",
       " 'tonight': 123,\n",
       " 'sorry': 124,\n",
       " 'bad': 125,\n",
       " \"'ve\": 126,\n",
       " 'right': 127,\n",
       " 'very': 128,\n",
       " 'by': 129,\n",
       " 'make': 130,\n",
       " 'should': 131,\n",
       " 'them': 132,\n",
       " 'nice': 133,\n",
       " 'though': 134,\n",
       " 'mother': 135,\n",
       " 'better': 136,\n",
       " 'gonna': 137,\n",
       " 'yeah': 138,\n",
       " 'sleep': 139,\n",
       " 'week': 140,\n",
       " 'way': 141,\n",
       " 'getting': 142,\n",
       " 'over': 143,\n",
       " 'weekend': 144,\n",
       " 'mothers': 145,\n",
       " 'come': 146,\n",
       " 'bit': 147,\n",
       " 'people': 148,\n",
       " 'bed': 149,\n",
       " 'twitpic': 150,\n",
       " 'were': 151,\n",
       " 'next': 152,\n",
       " 'does': 153,\n",
       " 'school': 154,\n",
       " 'dont': 155,\n",
       " 'watching': 156,\n",
       " 'lt': 157,\n",
       " 'awesome': 158,\n",
       " 'after': 159,\n",
       " 'days': 160,\n",
       " 'him': 161,\n",
       " 'mom': 162,\n",
       " 'wait': 163,\n",
       " 'hate': 164,\n",
       " 'hey': 165,\n",
       " 'even': 166,\n",
       " 'say': 167,\n",
       " 'best': 168,\n",
       " 'never': 169,\n",
       " '4': 170,\n",
       " 'yes': 171,\n",
       " 'thank': 172,\n",
       " 'little': 173,\n",
       " 'who': 174,\n",
       " 'long': 175,\n",
       " 'soon': 176,\n",
       " 'show': 177,\n",
       " 'working': 178,\n",
       " 'being': 179,\n",
       " 'down': 180,\n",
       " 'thing': 181,\n",
       " 'having': 182,\n",
       " 'ok': 183,\n",
       " 'his': 184,\n",
       " 'take': 185,\n",
       " 'please': 186,\n",
       " 'done': 187,\n",
       " 'cant': 188,\n",
       " 'any': 189,\n",
       " 'sure': 190,\n",
       " 'life': 191,\n",
       " 'where': 192,\n",
       " 'everyone': 193,\n",
       " 'always': 194,\n",
       " 'our': 195,\n",
       " '1': 196,\n",
       " 'may': 197,\n",
       " 'than': 198,\n",
       " 'sick': 199,\n",
       " 'look': 200,\n",
       " 'tired': 201,\n",
       " 'first': 202,\n",
       " 'doing': 203,\n",
       " 'wanna': 204,\n",
       " 'ur': 205,\n",
       " 'cool': 206,\n",
       " 'because': 207,\n",
       " 'movie': 208,\n",
       " 'let': 209,\n",
       " 'feeling': 210,\n",
       " 'guys': 211,\n",
       " 'find': 212,\n",
       " 'already': 213,\n",
       " 'x': 214,\n",
       " 'another': 215,\n",
       " 'friends': 216,\n",
       " 'phone': 217,\n",
       " 'made': 218,\n",
       " 'man': 219,\n",
       " 'watch': 220,\n",
       " 'us': 221,\n",
       " 'something': 222,\n",
       " 'hours': 223,\n",
       " 'yet': 224,\n",
       " 'trying': 225,\n",
       " 'p': 226,\n",
       " '5': 227,\n",
       " 'yay': 228,\n",
       " 'ever': 229,\n",
       " 'before': 230,\n",
       " 'house': 231,\n",
       " 'ready': 232,\n",
       " 'old': 233,\n",
       " 'looking': 234,\n",
       " 'friend': 235,\n",
       " 'pretty': 236,\n",
       " 'girl': 237,\n",
       " 'friday': 238,\n",
       " 'maybe': 239,\n",
       " 'thought': 240,\n",
       " 'sucks': 241,\n",
       " 'finally': 242,\n",
       " 'live': 243,\n",
       " 'wo': 244,\n",
       " 'damn': 245,\n",
       " 'went': 246,\n",
       " 'left': 247,\n",
       " 'guess': 248,\n",
       " 'away': 249,\n",
       " 'follow': 250,\n",
       " \"'d\": 251,\n",
       " 'same': 252,\n",
       " 'into': 253,\n",
       " 'someone': 254,\n",
       " 'n': 255,\n",
       " 'amazing': 256,\n",
       " 'big': 257,\n",
       " 'nothing': 258,\n",
       " 'monday': 259,\n",
       " 'looks': 260,\n",
       " 'ya': 261,\n",
       " 'other': 262,\n",
       " 'omg': 263,\n",
       " 'wow': 264,\n",
       " 'year': 265,\n",
       " 'also': 266,\n",
       " 'keep': 267,\n",
       " 'missed': 268,\n",
       " 'tweet': 269,\n",
       " 'while': 270,\n",
       " 'things': 271,\n",
       " 'two': 272,\n",
       " 'bored': 273,\n",
       " 'said': 274,\n",
       " 'star': 275,\n",
       " 'hot': 276,\n",
       " 'later': 277,\n",
       " 'those': 278,\n",
       " 'hear': 279,\n",
       " 'glad': 280,\n",
       " 'hard': 281,\n",
       " 'ly': 282,\n",
       " 'actually': 283,\n",
       " 'try': 284,\n",
       " 'tell': 285,\n",
       " 'ugh': 286,\n",
       " 'car': 287,\n",
       " 'baby': 288,\n",
       " 'saw': 289,\n",
       " 'early': 290,\n",
       " 'lost': 291,\n",
       " 'call': 292,\n",
       " 'such': 293,\n",
       " 'help': 294,\n",
       " 'world': 295,\n",
       " 'tinyurl': 296,\n",
       " 'b': 297,\n",
       " 'w': 298,\n",
       " 'sun': 299,\n",
       " 'job': 300,\n",
       " 'song': 301,\n",
       " 'coming': 302,\n",
       " 'weather': 303,\n",
       " 'hi': 304,\n",
       " 'head': 305,\n",
       " 'start': 306,\n",
       " 'makes': 307,\n",
       " 'around': 308,\n",
       " 'might': 309,\n",
       " 'birthday': 310,\n",
       " 'waiting': 311,\n",
       " 'until': 312,\n",
       " 'stuff': 313,\n",
       " 'rain': 314,\n",
       " 'gone': 315,\n",
       " 'play': 316,\n",
       " 'god': 317,\n",
       " 'gotta': 318,\n",
       " 'must': 319,\n",
       " 'poor': 320,\n",
       " 'few': 321,\n",
       " 'yesterday': 322,\n",
       " 'myself': 323,\n",
       " 'thats': 324,\n",
       " 'since': 325,\n",
       " 'party': 326,\n",
       " 'excited': 327,\n",
       " 'lot': 328,\n",
       " 'o': 329,\n",
       " 'music': 330,\n",
       " 'late': 331,\n",
       " 'making': 332,\n",
       " 'read': 333,\n",
       " 'anything': 334,\n",
       " 'hair': 335,\n",
       " 'check': 336,\n",
       " 'aww': 337,\n",
       " 'family': 338,\n",
       " 'their': 339,\n",
       " 'found': 340,\n",
       " 'summer': 341,\n",
       " 'give': 342,\n",
       " 'almost': 343,\n",
       " 'till': 344,\n",
       " 'put': 345,\n",
       " 'mean': 346,\n",
       " 'least': 347,\n",
       " 'enjoy': 348,\n",
       " 'sunday': 349,\n",
       " 'talk': 350,\n",
       " 'anyone': 351,\n",
       " 'everything': 352,\n",
       " 'welcome': 353,\n",
       " 'cute': 354,\n",
       " 'without': 355,\n",
       " 'leave': 356,\n",
       " 'lunch': 357,\n",
       " 'end': 358,\n",
       " 'r': 359,\n",
       " 'listening': 360,\n",
       " 'totally': 361,\n",
       " 's': 362,\n",
       " 'funny': 363,\n",
       " 'dinner': 364,\n",
       " 'hurts': 365,\n",
       " 'money': 366,\n",
       " 'tho': 367,\n",
       " '10': 368,\n",
       " 'game': 369,\n",
       " 'most': 370,\n",
       " 'thinking': 371,\n",
       " 'eat': 372,\n",
       " 'free': 373,\n",
       " 'wanted': 374,\n",
       " 'sounds': 375,\n",
       " '6': 376,\n",
       " 'stop': 377,\n",
       " 'mine': 378,\n",
       " 'hour': 379,\n",
       " 'many': 380,\n",
       " 'ha': 381,\n",
       " 'stupid': 382,\n",
       " 'm': 383,\n",
       " 'www': 384,\n",
       " 'far': 385,\n",
       " 'missing': 386,\n",
       " 'forward': 387,\n",
       " 'beautiful': 388,\n",
       " 'use': 389,\n",
       " 'finished': 390,\n",
       " 'luck': 391,\n",
       " 'probably': 392,\n",
       " 'believe': 393,\n",
       " 'cold': 394,\n",
       " 'playing': 395,\n",
       " 'd': 396,\n",
       " 'food': 397,\n",
       " 'cause': 398,\n",
       " 'through': 399,\n",
       " 'place': 400,\n",
       " 'gt': 401,\n",
       " 'sweet': 402,\n",
       " 'times': 403,\n",
       " 'coffee': 404,\n",
       " 'xx': 405,\n",
       " 'okay': 406,\n",
       " 'these': 407,\n",
       " 'shit': 408,\n",
       " 'which': 409,\n",
       " 'lovely': 410,\n",
       " 'every': 411,\n",
       " 't': 412,\n",
       " 'hahaha': 413,\n",
       " 'didnt': 414,\n",
       " 'eating': 415,\n",
       " 'outside': 416,\n",
       " 'enough': 417,\n",
       " 'real': 418,\n",
       " 'ill': 419,\n",
       " 'followers': 420,\n",
       " 'c': 421,\n",
       " 'plurk': 422,\n",
       " 'stuck': 423,\n",
       " 'kids': 424,\n",
       " 'says': 425,\n",
       " 'office': 426,\n",
       " 'kinda': 427,\n",
       " 'awww': 428,\n",
       " 'took': 429,\n",
       " 'headache': 430,\n",
       " 'whole': 431,\n",
       " 'sooo': 432,\n",
       " 'seen': 433,\n",
       " 'room': 434,\n",
       " 'pic': 435,\n",
       " '30': 436,\n",
       " '8': 437,\n",
       " '7': 438,\n",
       " 'woke': 439,\n",
       " 'came': 440,\n",
       " 'wants': 441,\n",
       " 'buy': 442,\n",
       " 'wrong': 443,\n",
       " 'anymore': 444,\n",
       " 'dad': 445,\n",
       " 'tv': 446,\n",
       " 'years': 447,\n",
       " 'following': 448,\n",
       " 'weeks': 449,\n",
       " 'video': 450,\n",
       " 'stay': 451,\n",
       " 'moms': 452,\n",
       " 'forgot': 453,\n",
       " 'guy': 454,\n",
       " 'full': 455,\n",
       " 'name': 456,\n",
       " 'ago': 457,\n",
       " 'both': 458,\n",
       " 'taking': 459,\n",
       " 'goodnight': 460,\n",
       " 'boo': 461,\n",
       " 'meet': 462,\n",
       " 'else': 463,\n",
       " 'super': 464,\n",
       " 'book': 465,\n",
       " 'loved': 466,\n",
       " 'busy': 467,\n",
       " 'sitting': 468,\n",
       " 'able': 469,\n",
       " 'class': 470,\n",
       " 'shopping': 471,\n",
       " 'mum': 472,\n",
       " 'hopefully': 473,\n",
       " 'youtube': 474,\n",
       " 'alone': 475,\n",
       " 'tweets': 476,\n",
       " 'half': 477,\n",
       " 'post': 478,\n",
       " 'facebook': 479,\n",
       " 'iphone': 480,\n",
       " 'lots': 481,\n",
       " 'online': 482,\n",
       " 'seems': 483,\n",
       " 'used': 484,\n",
       " 'hell': 485,\n",
       " 'internet': 486,\n",
       " 'own': 487,\n",
       " 'hit': 488,\n",
       " 'cry': 489,\n",
       " 'idea': 490,\n",
       " 'trek': 491,\n",
       " 'break': 492,\n",
       " 'news': 493,\n",
       " 'quite': 494,\n",
       " 'told': 495,\n",
       " 'holiday': 496,\n",
       " 'gets': 497,\n",
       " 'called': 498,\n",
       " 'run': 499,\n",
       " 'send': 500,\n",
       " 'dog': 501,\n",
       " 'hello': 502,\n",
       " 'wars': 503,\n",
       " 'talking': 504,\n",
       " 'computer': 505,\n",
       " 'remember': 506,\n",
       " 'dude': 507,\n",
       " 'hurt': 508,\n",
       " 'boy': 509,\n",
       " 'btw': 510,\n",
       " 'hehe': 511,\n",
       " 'crazy': 512,\n",
       " 'saturday': 513,\n",
       " 'change': 514,\n",
       " 'wont': 515,\n",
       " 'seeing': 516,\n",
       " 'cuz': 517,\n",
       " 'win': 518,\n",
       " 'ah': 519,\n",
       " 'once': 520,\n",
       " 'either': 521,\n",
       " 'minutes': 522,\n",
       " 'tried': 523,\n",
       " 'fucking': 524,\n",
       " 'bank': 525,\n",
       " 'rest': 526,\n",
       " 'fm': 527,\n",
       " 'face': 528,\n",
       " 'raining': 529,\n",
       " 'awake': 530,\n",
       " 'feels': 531,\n",
       " 'blog': 532,\n",
       " 'kind': 533,\n",
       " 'watched': 534,\n",
       " 'heard': 535,\n",
       " 'lucky': 536,\n",
       " 'picture': 537,\n",
       " 'drive': 538,\n",
       " 'broke': 539,\n",
       " 'soo': 540,\n",
       " 'breakfast': 541,\n",
       " 'wonderful': 542,\n",
       " 'part': 543,\n",
       " 'starting': 544,\n",
       " 'blip': 545,\n",
       " 'true': 546,\n",
       " 'evening': 547,\n",
       " 'site': 548,\n",
       " 'lmao': 549,\n",
       " 'instead': 550,\n",
       " 'girls': 551,\n",
       " 'mind': 552,\n",
       " 'trip': 553,\n",
       " 'beach': 554,\n",
       " 'person': 555,\n",
       " 'high': 556,\n",
       " 'heart': 557,\n",
       " 'link': 558,\n",
       " 'drink': 559,\n",
       " 'leaving': 560,\n",
       " 'concert': 561,\n",
       " 'pain': 562,\n",
       " 'sometimes': 563,\n",
       " 'perfect': 564,\n",
       " 'fail': 565,\n",
       " 'aw': 566,\n",
       " 'months': 567,\n",
       " 'la': 568,\n",
       " 'mommy': 569,\n",
       " 'ice': 570,\n",
       " 'dead': 571,\n",
       " 'close': 572,\n",
       " 'set': 573,\n",
       " 'fine': 574,\n",
       " 'suck': 575,\n",
       " 'fuck': 576,\n",
       " 'turn': 577,\n",
       " 'bought': 578,\n",
       " 'move': 579,\n",
       " 'soooo': 580,\n",
       " 'hungry': 581,\n",
       " 'loves': 582,\n",
       " 'wishing': 583,\n",
       " 'y': 584,\n",
       " 'goes': 585,\n",
       " 'til': 586,\n",
       " 'month': 587,\n",
       " 'yea': 588,\n",
       " 'card': 589,\n",
       " 'hoping': 590,\n",
       " 'ask': 591,\n",
       " 'shower': 592,\n",
       " 'anyway': 593,\n",
       " 'writing': 594,\n",
       " 'e': 595,\n",
       " 'happened': 596,\n",
       " 'heading': 597,\n",
       " 'ones': 598,\n",
       " 'care': 599,\n",
       " 'fan': 600,\n",
       " 'june': 601,\n",
       " 'running': 602,\n",
       " 'started': 603,\n",
       " 'definitely': 604,\n",
       " 'favorite': 605,\n",
       " 'boring': 606,\n",
       " 'tea': 607,\n",
       " 'problem': 608,\n",
       " 'lil': 609,\n",
       " 'ipod': 610,\n",
       " 'seriously': 611,\n",
       " 'exam': 612,\n",
       " 'xd': 613,\n",
       " 'nite': 614,\n",
       " 'needs': 615,\n",
       " 'comes': 616,\n",
       " 'test': 617,\n",
       " 'uk': 618,\n",
       " 'couple': 619,\n",
       " 'sunny': 620,\n",
       " 'cat': 621,\n",
       " 'reading': 622,\n",
       " 'pics': 623,\n",
       " 'crap': 624,\n",
       " 'reply': 625,\n",
       " 'asleep': 626,\n",
       " 'bout': 627,\n",
       " 'write': 628,\n",
       " 'list': 629,\n",
       " '4th': 630,\n",
       " 'wake': 631,\n",
       " 'open': 632,\n",
       " 'sigh': 633,\n",
       " 'course': 634,\n",
       " '9': 635,\n",
       " 'moving': 636,\n",
       " 'city': 637,\n",
       " 'moment': 638,\n",
       " 'tickets': 639,\n",
       " 'chocolate': 640,\n",
       " 'sore': 641,\n",
       " 'movies': 642,\n",
       " 'ass': 643,\n",
       " 'means': 644,\n",
       " 'less': 645,\n",
       " 'jealous': 646,\n",
       " 'email': 647,\n",
       " 'top': 648,\n",
       " 'text': 649,\n",
       " 'songs': 650,\n",
       " 'catch': 651,\n",
       " 'ive': 652,\n",
       " 'yep': 653,\n",
       " 'short': 654,\n",
       " 'fast': 655,\n",
       " 'supposed': 656,\n",
       " 'eyes': 657,\n",
       " 'mad': 658,\n",
       " 'died': 659,\n",
       " 'laptop': 660,\n",
       " 'meeting': 661,\n",
       " 'gave': 662,\n",
       " 'dream': 663,\n",
       " 'works': 664,\n",
       " 'sleeping': 665,\n",
       " 'town': 666,\n",
       " 'bye': 667,\n",
       " 'rock': 668,\n",
       " 'tommcfly': 669,\n",
       " 'reason': 670,\n",
       " 'red': 671,\n",
       " 'special': 672,\n",
       " 'spend': 673,\n",
       " 'photo': 674,\n",
       " 'happen': 675,\n",
       " 'together': 676,\n",
       " 'dear': 677,\n",
       " 'vote': 678,\n",
       " 'fb': 679,\n",
       " 'final': 680,\n",
       " 'saying': 681,\n",
       " 'goin': 682,\n",
       " 'cleaning': 683,\n",
       " 'sister': 684,\n",
       " 'cream': 685,\n",
       " 'cut': 686,\n",
       " '12': 687,\n",
       " 'hmm': 688,\n",
       " 'account': 689,\n",
       " 'sadly': 690,\n",
       " 'hugs': 691,\n",
       " 'season': 692,\n",
       " 'using': 693,\n",
       " 'tour': 694,\n",
       " 'boys': 695,\n",
       " 're': 696,\n",
       " 'due': 697,\n",
       " 'gym': 698,\n",
       " 'interesting': 699,\n",
       " 'forget': 700,\n",
       " 'xxx': 701,\n",
       " 'black': 702,\n",
       " 'enjoying': 703,\n",
       " 'hang': 704,\n",
       " 'afternoon': 705,\n",
       " 'water': 706,\n",
       " 'exams': 707,\n",
       " 'nap': 708,\n",
       " 'bring': 709,\n",
       " 'sound': 710,\n",
       " 'driving': 711,\n",
       " '1st': 712,\n",
       " 'date': 713,\n",
       " 'mitchelmusso': 714,\n",
       " 'lady': 715,\n",
       " 'broken': 716,\n",
       " 'pictures': 717,\n",
       " '11': 718,\n",
       " 'second': 719,\n",
       " 'brother': 720,\n",
       " 'knew': 721,\n",
       " '20': 722,\n",
       " 'finish': 723,\n",
       " 'plans': 724,\n",
       " 'l': 725,\n",
       " 'listen': 726,\n",
       " 'past': 727,\n",
       " 'ride': 728,\n",
       " 'google': 729,\n",
       " 'clean': 730,\n",
       " 'warm': 731,\n",
       " 'unfortunately': 732,\n",
       " 'fell': 733,\n",
       " 'da': 734,\n",
       " 'flu': 735,\n",
       " 'pizza': 736,\n",
       " 'spent': 737,\n",
       " 'weird': 738,\n",
       " 'mr': 739,\n",
       " 'nope': 740,\n",
       " 'ppl': 741,\n",
       " 'myspace': 742,\n",
       " 'photos': 743,\n",
       " 'episode': 744,\n",
       " 'worth': 745,\n",
       " 'doesnt': 746,\n",
       " 'park': 747,\n",
       " 'horrible': 748,\n",
       " 'agree': 749,\n",
       " 'walk': 750,\n",
       " 'page': 751,\n",
       " 'seem': 752,\n",
       " 'upset': 753,\n",
       " 'kid': 754,\n",
       " 'bgt': 755,\n",
       " 'london': 756,\n",
       " 'ate': 757,\n",
       " 'ahh': 758,\n",
       " 'under': 759,\n",
       " 'liked': 760,\n",
       " 'jonas': 761,\n",
       " 'sent': 762,\n",
       " 'sunshine': 763,\n",
       " 'three': 764,\n",
       " 'mood': 765,\n",
       " 'different': 766,\n",
       " 'drinking': 767,\n",
       " 'bday': 768,\n",
       " 'mileycyrus': 769,\n",
       " 'beer': 770,\n",
       " 'k': 771,\n",
       " 'club': 772,\n",
       " 'david': 773,\n",
       " 'dance': 774,\n",
       " 'slow': 775,\n",
       " 'especially': 776,\n",
       " 'side': 777,\n",
       " '15': 778,\n",
       " 'cake': 779,\n",
       " \"'\": 780,\n",
       " 'plan': 781,\n",
       " 'add': 782,\n",
       " 'wtf': 783,\n",
       " 'garden': 784,\n",
       " 'fair': 785,\n",
       " 'shows': 786,\n",
       " 'rather': 787,\n",
       " 'congrats': 788,\n",
       " 'fall': 789,\n",
       " 'study': 790,\n",
       " 'store': 791,\n",
       " 'inside': 792,\n",
       " 'wedding': 793,\n",
       " 'bus': 794,\n",
       " 'huge': 795,\n",
       " 'hug': 796,\n",
       " 'worse': 797,\n",
       " 'words': 798,\n",
       " 'chance': 799,\n",
       " 'mama': 800,\n",
       " 'son': 801,\n",
       " 'die': 802,\n",
       " 'except': 803,\n",
       " 'message': 804,\n",
       " 'played': 805,\n",
       " '100': 806,\n",
       " 'fans': 807,\n",
       " 'story': 808,\n",
       " 'english': 809,\n",
       " 'save': 810,\n",
       " 'wear': 811,\n",
       " 'meant': 812,\n",
       " 'point': 813,\n",
       " 'havent': 814,\n",
       " 'apparently': 815,\n",
       " 'ahhh': 816,\n",
       " 'babe': 817,\n",
       " 'green': 818,\n",
       " 'white': 819,\n",
       " 'worry': 820,\n",
       " 'learn': 821,\n",
       " 'album': 822,\n",
       " 'visit': 823,\n",
       " 'update': 824,\n",
       " 'plus': 825,\n",
       " 'looked': 826,\n",
       " 'completely': 827,\n",
       " 'loving': 828,\n",
       " 'shame': 829,\n",
       " 'figure': 830,\n",
       " 'whats': 831,\n",
       " 'hand': 832,\n",
       " 'smile': 833,\n",
       " 'profile': 834,\n",
       " 'throat': 835,\n",
       " 'easy': 836,\n",
       " 'word': 837,\n",
       " 'answer': 838,\n",
       " 'everybody': 839,\n",
       " 'earlier': 840,\n",
       " 'via': 841,\n",
       " 'slept': 842,\n",
       " 'tweeting': 843,\n",
       " 'worst': 844,\n",
       " 'v': 845,\n",
       " 'sat': 846,\n",
       " 'idk': 847,\n",
       " 'yours': 848,\n",
       " 'happens': 849,\n",
       " 'traffic': 850,\n",
       " 'thx': 851,\n",
       " 'yummy': 852,\n",
       " 'snl': 853,\n",
       " 'prom': 854,\n",
       " '0': 855,\n",
       " 'living': 856,\n",
       " 'dang': 857,\n",
       " 'mac': 858,\n",
       " 'updates': 859,\n",
       " 'guitar': 860,\n",
       " 'dreams': 861,\n",
       " 'men': 862,\n",
       " 'needed': 863,\n",
       " 'lame': 864,\n",
       " 'drunk': 865,\n",
       " 'absolutely': 866,\n",
       " 'bet': 867,\n",
       " 'ohh': 868,\n",
       " 'line': 869,\n",
       " 'business': 870,\n",
       " 'project': 871,\n",
       " 'during': 872,\n",
       " 'kill': 873,\n",
       " 'crying': 874,\n",
       " 'near': 875,\n",
       " 'whatever': 876,\n",
       " 'relaxing': 877,\n",
       " 'yum': 878,\n",
       " 'longer': 879,\n",
       " 'film': 880,\n",
       " 'band': 881,\n",
       " 'each': 882,\n",
       " 'won': 883,\n",
       " 'gift': 884,\n",
       " 'starts': 885,\n",
       " 'radio': 886,\n",
       " 'tom': 887,\n",
       " 'sign': 888,\n",
       " 'parents': 889,\n",
       " 'camera': 890,\n",
       " 'air': 891,\n",
       " 'along': 892,\n",
       " 'blue': 893,\n",
       " 'apple': 894,\n",
       " 'wondering': 895,\n",
       " 'met': 896,\n",
       " '2day': 897,\n",
       " 'daughter': 898,\n",
       " 'homework': 899,\n",
       " 'felt': 900,\n",
       " 'ddlovato': 901,\n",
       " 'net': 902,\n",
       " 'wonder': 903,\n",
       " 'number': 904,\n",
       " 'lazy': 905,\n",
       " 'vegas': 906,\n",
       " '2nd': 907,\n",
       " 'chat': 908,\n",
       " 'hun': 909,\n",
       " 'luv': 910,\n",
       " 'company': 911,\n",
       " 'bike': 912,\n",
       " 'hubby': 913,\n",
       " 'feet': 914,\n",
       " 'bless': 915,\n",
       " 'cannot': 916,\n",
       " 'hanging': 917,\n",
       " 'posted': 918,\n",
       " 'wishes': 919,\n",
       " 'shoes': 920,\n",
       " 'shirt': 921,\n",
       " 'fingers': 922,\n",
       " 'thinks': 923,\n",
       " 'although': 924,\n",
       " 'yourself': 925,\n",
       " 'woo': 926,\n",
       " 'hmmm': 927,\n",
       " 'paper': 928,\n",
       " 'realized': 929,\n",
       " 'wife': 930,\n",
       " 'stomach': 931,\n",
       " 'hates': 932,\n",
       " 'sexy': 933,\n",
       " 'lonely': 934,\n",
       " 'small': 935,\n",
       " 'behind': 936,\n",
       " 'nick': 937,\n",
       " 'cheese': 938,\n",
       " 'comment': 939,\n",
       " 'chicken': 940,\n",
       " 'fact': 941,\n",
       " 'join': 942,\n",
       " 'fantastic': 943,\n",
       " 'ff': 944,\n",
       " 'staying': 945,\n",
       " 'forever': 946,\n",
       " 'worked': 947,\n",
       " 'understand': 948,\n",
       " 'graduation': 949,\n",
       " 'justin': 950,\n",
       " 'windows': 951,\n",
       " 'wine': 952,\n",
       " 'app': 953,\n",
       " 'church': 954,\n",
       " 'body': 955,\n",
       " 'dm': 956,\n",
       " 'version': 957,\n",
       " 'fly': 958,\n",
       " 'safe': 959,\n",
       " 'shop': 960,\n",
       " 'sleepy': 961,\n",
       " 'middle': 962,\n",
       " 'aint': 963,\n",
       " 'quick': 964,\n",
       " 'ouch': 965,\n",
       " 'officially': 966,\n",
       " 'ooh': 967,\n",
       " 'sold': 968,\n",
       " 'pool': 969,\n",
       " 'pick': 970,\n",
       " 'single': 971,\n",
       " 'usually': 972,\n",
       " 'college': 973,\n",
       " 'bbq': 974,\n",
       " 'pass': 975,\n",
       " 'brothers': 976,\n",
       " 'mobile': 977,\n",
       " 'minute': 978,\n",
       " 'ran': 979,\n",
       " 'boyfriend': 980,\n",
       " 'power': 981,\n",
       " 'taken': 982,\n",
       " 'cos': 983,\n",
       " 'chinese': 984,\n",
       " 'keeps': 985,\n",
       " 'dvd': 986,\n",
       " 'studying': 987,\n",
       " 'cup': 988,\n",
       " 'john': 989,\n",
       " 'tummy': 990,\n",
       " 'g': 991,\n",
       " 'proud': 992,\n",
       " 'tuesday': 993,\n",
       " 'team': 994,\n",
       " 'clothes': 995,\n",
       " 'tr': 996,\n",
       " 'deal': 997,\n",
       " 'thursday': 998,\n",
       " 'appreciate': 999,\n",
       " 'walking': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   28,  184,  543],\n",
       "       [   0,    0,    0, ...,   17,   43,  292],\n",
       "       [   0,    0,    0, ..., 4176, 1745,  238],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    9,   43,   23],\n",
       "       [   0,    0,    0, ...,   10,    3,  450],\n",
       "       [   0,    0,    0, ...,  325,  998, 1234]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstmModel(embedding_mat,40,no_of_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(embedding_mat,40,no_of_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('dumped_model.pk', 'wb') as file:\n",
    "#     pickle.dump(model,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "# model.save(\"lstm_model\")\n",
    "model1 = keras.models.load_model(\"lstm_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = ['anger', 'boredom', 'empty', 'enthusiasm', 'fun', \n",
    "                  'happiness', 'hate', 'love', 'neutral', 'relief', \n",
    "                  'sadness', 'surprise', 'worry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 140ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sadness'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing \n",
    "\n",
    "test_sentence = [\"I am feeling sad\"]\n",
    "test_data = oneSentence(test_sentence, tokenizer)\n",
    "\n",
    "emo = model1.predict(test_data[1])\n",
    "g = emo[0][0]\n",
    "index = 0\n",
    "for i in range(1,len(label_dict)):\n",
    "    if emo[0][i] > g:\n",
    "        g = emo[0][i]\n",
    "        index = i\n",
    "\n",
    "\n",
    "label_dict[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger',\n",
       " 'boredom',\n",
       " 'empty',\n",
       " 'enthusiasm',\n",
       " 'fun',\n",
       " 'happiness',\n",
       " 'hate',\n",
       " 'love',\n",
       " 'neutral',\n",
       " 'relief',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'worry']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final project adds -\n",
    " > Movie recommender system\n",
    " \n",
    " > Combine both the chatbot and movie recommneder\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('tensorflowenv_20220122')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "c2e8fcb74b459646169e5c12b1b3bad8bafdc0d857aabfcf8ac08160d16192a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
